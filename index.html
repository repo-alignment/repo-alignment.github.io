<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RE-PO | Robust Enhanced Policy Optimization for LLM Alignment</title>
  <meta name="description" content="RE-PO is an ICLR 2026 conference paper on robust LLM preference optimization under noisy labels.">
  <meta name="theme-color" content="#0f172a">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:wght@400;600;700&family=Space+Grotesk:wght@400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="assets/css/site.css">
</head>
<body>
  <a class="skip-link" href="#main-content">Skip to main content</a>

  <header class="topbar">
    <div class="container topbar-inner">
      <p class="project-kicker" id="project-kicker">ICLR 2026</p>
      <nav class="topnav" aria-label="Primary">
        <a href="#tldr">TL;DR</a>
        <a href="#method">Method</a>
        <a href="#results">Results</a>
        <a href="#citation">Citation</a>
      </nav>
    </div>
  </header>

  <main id="main-content">
    <section class="hero reveal" aria-labelledby="hero-title">
      <div class="container hero-grid">
        <div>
          <p class="paper-badge" id="paper-badge">ICLR 2026 Conference Paper</p>
          <p class="hero-tag">Project Page</p>
          <h1 id="hero-title">RE-PO: Robust Enhanced Policy Optimization for LLM Alignment</h1>
          <p class="hero-subtitle" id="hero-tagline">
            Robust preference optimization under noisy labels with EM-based reliability weighting.
          </p>
          <p class="hero-quant">
            Conference publication at ICLR 2026.
          </p>
          <div class="hero-cta" role="group" aria-label="Project links">
            <a id="cta-paper" class="btn btn-primary" href="assets/paper/iclr2026_conference.pdf" target="_blank" rel="noopener">Paper</a>
            <a id="cta-code" class="btn btn-secondary" href="https://github.com/xycao/RE-PO/tree/main/re-po-open" target="_blank" rel="noopener">Code</a>
            <a id="cta-citation" class="btn btn-ghost" href="#citation">Citation</a>
          </div>
          <p class="author-line" id="author-line">
            Xiaoyang Cao, Zelai Xu, Mo Guang, Kaiwen Long, Michiel A. Bakker, Yu Wang, Chao Yu
          </p>
          <p class="affiliation-line" id="affiliation-line">
            Massachusetts Institute of Technology · Tsinghua University · Li Auto Inc.
          </p>
        </div>
        <aside class="hero-aside" aria-label="Key facts">
          <div class="fact-card">
            <h2>Core Idea</h2>
            <p>Infer preference-label correctness and annotator reliability, then reweight loss during policy optimization.</p>
          </div>
          <div class="fact-card">
            <h2>Scope</h2>
            <p>UltraFeedback and MultiPref experiments on Mistral-7B and Llama-3-8B.</p>
          </div>
          <div class="fact-card">
            <h2>Target Metrics</h2>
            <p>AlpacaEval 2 Length-Controlled Win Rate (LC) and Win Rate (WR).</p>
          </div>
        </aside>
      </div>
    </section>

    <section id="tldr" class="section reveal" aria-labelledby="tldr-title">
      <div class="container">
        <h2 id="tldr-title">TL;DR</h2>
        <ul class="tldr-list">
          <li><strong>Research Gap:</strong> Preference labels are often noisy, while many alignment methods assume uniform label reliability.</li>
          <li><strong>Method:</strong> RE-PO uses an EM-style procedure to estimate label confidence and annotator reliability, then applies adaptive loss weighting.</li>
          <li><strong>Performance:</strong> RE-PO improves AlpacaEval 2 LC/WR over DPO across UltraFeedback and MultiPref setups.</li>
        </ul>
      </div>
    </section>

    <section id="method" class="section section-alt reveal" aria-labelledby="method-title">
      <div class="container method-grid">
        <div>
          <h2 id="method-title">Method Overview</h2>
          <p>
            RE-PO models observed preferences as potentially noisy signals. In the E-step, it estimates posterior confidence that
            each label is correct. In the M-step, it updates policy parameters and annotator reliability with these confidences as weights.
          </p>
          <p>
            This keeps the training pipeline close to DPO while improving robustness against corrupted or inconsistent feedback.
          </p>
          <div class="mini-note" role="note">
            Practical training uses mini-batch compatible reliability updates.
          </div>
        </div>
        <figure class="figure-card static-method-figure">
          <img src="assets/img/flow_chart.png" alt="Flow chart of RE-PO EM loop from noisy labels to weighted policy optimization.">
          <figcaption>RE-PO alternates confidence estimation and weighted policy updates.</figcaption>
        </figure>
      </div>
    </section>

    <section id="results" class="section reveal" aria-labelledby="results-title">
      <div class="container">
        <h2 id="results-title">Key Results</h2>
        <p class="section-lead">
          On UltraFeedback and MultiPref, RE-DPO improves LC and WR over DPO for both Mistral-7B and Llama-3-8B.
        </p>
        <div class="table-wrap" role="region" aria-label="Main results" tabindex="0">
          <table id="results-table">
            <thead>
              <tr>
                <th>Dataset</th>
                <th>Model</th>
                <th>DPO (LC/WR)</th>
                <th>RE-DPO (LC/WR)</th>
                <th>Delta LC</th>
                <th>Delta WR</th>
              </tr>
            </thead>
            <tbody id="results-body"></tbody>
          </table>
        </div>
      </div>
    </section>

    <section id="citation" class="section reveal" aria-labelledby="citation-title">
      <div class="container citation-grid">
        <div>
          <h2 id="citation-title">Citation</h2>
          <p>If RE-PO is useful to your research, please cite:</p>
          <div class="citation-box">
            <pre id="citation-text"><code>@inproceedings{cao2026repo,
  title     = {RE-PO: Robust Enhanced Policy Optimization as a General Framework for LLM Alignment},
  author    = {Cao, Xiaoyang and Xu, Zelai and Guang, Mo and Long, Kaiwen and Bakker, Michiel A. and Wang, Yu and Yu, Chao},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2026}
}</code></pre>
            <button id="copy-citation" class="btn btn-secondary" type="button">Copy BibTeX</button>
            <p id="copy-status" class="copy-status" aria-live="polite"></p>
          </div>
        </div>
        <aside class="contact-card" aria-label="Contact and links">
          <h3>Contact</h3>
          <p>Email: <a id="contact-email" href="mailto:xycao@mit.edu">xycao@mit.edu</a></p>
          <p><a id="contact-code-link" href="https://github.com/xycao/RE-PO/tree/main/re-po-open" target="_blank" rel="noopener">Code Repository</a></p>
        </aside>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container footer-inner">
      <p><span id="project-name">RE-PO</span> <span id="project-year">2026</span></p>
      <p class="footer-note">License: <span id="project-license">Apache-2.0</span></p>
    </div>
  </footer>

  <script src="assets/js/site.js" defer></script>
</body>
</html>
